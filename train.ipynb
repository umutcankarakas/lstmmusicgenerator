{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_reduced.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1KNAvoQhjYkJGgivSG7UxViJVSihcx9Mh",
      "authorship_tag": "ABX9TyODuykQlIJAep5pBSArfWdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umutcankarakas/lstmmusicgenerator/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVvFUC7jweP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3c31911b-4bef-4228-e720-2f0dbf322ebf"
      },
      "source": [
        "!git clone https://github.com/umutcankarakas/theorygenerator.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'theorygenerator'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 51 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0OZBiO-vWX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "bfe6956f-a329-4f18-b016-9ea7a388e707"
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=c34af8d65a0f0ea86204d71c9489b81e65235bcab791cfaecee19623d6327367\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.46.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_zvyBGIwwrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b135957e-ac68-46c6-e3a1-6f3fa87de3be"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import *\n",
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten\n",
        "#from keras import utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from matplotlib import pyplot as plt\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFkXiPHmwzlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('theorygenerator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8mUl6FWw2NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '../drive/My Drive/full_set_beethoven_mozart/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd7Q9-H2w5zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network():\n",
        "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
        "    notes = get_notes()\n",
        "\n",
        "    # get amount of pitch names\n",
        "    n_vocab = 128 + 128 + 2\n",
        "\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    model = create_network(network_input, n_vocab)\n",
        "    history = train(model, network_input, network_output)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56DmqL0ww69w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertMidi2Arr(midi_element):\n",
        "  newlist = []\n",
        "  max_duration = 8*4*4\n",
        "  if isinstance(midi_element, note.Note):\n",
        "    newlist.append(midi_element.pitch.midi)\n",
        "    if int(round(midi_element.duration.quarterLength*4)) > max_duration:\n",
        "      newlist.append(128 + max_duration)\n",
        "    else:\n",
        "      newlist.append(128 + int(round(midi_element.duration.quarterLength*4)))\n",
        "  elif isinstance(midi_element, chord.Chord):\n",
        "    for n in midi_element:\n",
        "      newlist.append(n.pitch.midi) \n",
        "      if int(round(midi_element.duration.quarterLength*4)) > max_duration:\n",
        "        newlist.append(128 + max_duration)\n",
        "      else:\n",
        "        newlist.append(128 + int(round(midi_element.duration.quarterLength*4)))\n",
        "  elif isinstance(midi_element, note.Rest):\n",
        "    newlist.append(128)\n",
        "    if int(round(midi_element.duration.quarterLength*4)) > max_duration:\n",
        "      newlist.append(128 + max_duration)\n",
        "    else:\n",
        "      newlist.append(128 + int(round(midi_element.duration.quarterLength*4)))\n",
        "  return newlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mlMr18Ww9Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_notes():\n",
        "  mylist = []\n",
        "  count = 0\n",
        "  last_offset = 0.0\n",
        "  offset_diff = 0.0\n",
        "  for file in glob.glob(train_path + '**/*.mid', recursive=True):\n",
        "    try:\n",
        "      midi = converter.parse(file)\n",
        "    except:\n",
        "      continue\n",
        "    #print(\"Parsing %s\" % file)\n",
        "    #print(count)\n",
        "    notes_to_parse = None\n",
        "    max_duration = 8*4*4\n",
        "\n",
        "    try: # file has instrument parts\n",
        "      s2 = instrument.partitionByInstrument(midi_piece)\n",
        "      notes_to_parse = s2.parts[0].recurse() \n",
        "    except: # file has notes in a flat structure\n",
        "      notes_to_parse = midi.flat.notesAndRests\n",
        "\n",
        "    same_offset_elements = []\n",
        "    #min_duration = max_duration\n",
        "    for element in notes_to_parse:\n",
        "      if isinstance(element, note.Note) or isinstance(element, chord.Chord) or isinstance(element, note.Rest):\n",
        "        if( not same_offset_elements or same_offset_elements[0].offset == element.offset):\n",
        "          same_offset_elements.append(element)\n",
        "        else:\n",
        "          add_arr = []\n",
        "          #flags = [0,0,0]\n",
        "          for toadd_midi in same_offset_elements:\n",
        "            toadd = convertMidi2Arr(toadd_midi)\n",
        "            if toadd[0]==128:\n",
        "              continue\n",
        "            add_arr.extend(toadd)\n",
        "          if(element.offset > last_offset):\n",
        "            offset_diff = same_offset_elements[0].offset - last_offset\n",
        "            last_offset = same_offset_elements[0].offset\n",
        "          if mylist and mylist[-1] != '_':\n",
        "            mylist.append(257)\n",
        "            mylist.append(128+int(round(offset_diff*4)))\n",
        "            mylist.extend(add_arr)\n",
        "          else:\n",
        "            mylist.extend(add_arr)\n",
        "\n",
        "          offset_diff = 0.0\n",
        "          same_offset_elements = []\n",
        "          same_offset_elements.append(element)\n",
        "\n",
        "    if same_offset_elements:\n",
        "      add_arr = []\n",
        "      for toadd_midi in same_offset_elements:\n",
        "        toadd = convertMidi2Arr(toadd_midi)\n",
        "        if toadd[0]==128:\n",
        "          continue\n",
        "        add_arr.extend(toadd)\n",
        "      if(element.offset > last_offset):\n",
        "        offset_diff = same_offset_elements[0].offset - last_offset\n",
        "        last_offset = same_offset_elements[0].offset\n",
        "      if mylist and mylist[-1] != '_':\n",
        "        mylist.append(257)\n",
        "        mylist.append(128+int(round(offset_diff*4)))\n",
        "        mylist.extend(add_arr)\n",
        "      else:\n",
        "        mylist.extend(add_arr)\n",
        "\n",
        "    if count != 0: count = count + 1\n",
        "    else: break\n",
        "    mylist.append('_')\n",
        "\n",
        "  with open('../drive/My Drive/data/notes_attention_mozbet', 'wb') as filepath:\n",
        "    pickle.dump(mylist, filepath)\n",
        "  return mylist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0anUuRPwxA2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 400\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        if '_' in sequence_in:\n",
        "          i = sequence_in.index('_') + 1\n",
        "          continue\n",
        "        elif sequence_out == '_':\n",
        "          i = i + sequence_length +1\n",
        "          continue\n",
        "        \n",
        "        network_input.append([element for element in sequence_in])\n",
        "        network_output.append(sequence_out)\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPVFuzn-ZW0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_1(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/bach_poins/weghts-improvement-292-0.2710-bigger.hdf5')): model.load_weights('../drive/My Drive/bach_points/weights-improvement-292-0.2710-bigger.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkNFawPNmxJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_2(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
        "        return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(LSTM(512,return_sequences=True))\n",
        "    model.add(Flatten()) #Supposedly needed to fix stuff before dense layer\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/bach_points/weghts-improvement-292-0.2710-bigger.hdf5')): model.load_weights('../drive/My Drive/bach_points/weights-improvement-292-0.2710-bigger.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC4D-wmszYmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_3(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/checkpoints_6lstm_se400only_t/weights-024-0.8723.hdf5')): model.load_weights('../drive/My Drive/checkpoints_6lstm_seq400_only_t/weights-024-0.8723.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFHT1UYbvYmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_4(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        recurrent_dropout=0.2,\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.2,))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/bach_poins/weghts-improvement-292-0.2710-bigger.hdf5')): model.load_weights('../drive/My Drive/bach_points/weights-improvement-292-0.2710-bigger.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMXGljnU3Khv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_5(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
        "        return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(LSTM(512,return_sequences=True))\n",
        "    model.add(Flatten()) #Supposedly needed to fix stuff before dense layer\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/bach_ponts/weghts-improvement-292-0.2710-bigger.hdf5')): model.load_weights('../drive/My Drive/bach_points/weights-improvement-292-0.2710-bigger.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPo6jHSP5z_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
        "        return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    #model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(LSTM(512,return_sequences=True))\n",
        "    #model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Flatten()) #Supposedly needed to fix stuff before dense layer\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/checkpoints_attentio/weights-00-0.0274.hdf5')): model.load_weights('../drive/My Drive/checkpoints_attention/weights-200-0.0274.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQhqxV0N1U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network_7(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
        "        return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(LSTM(512,return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Flatten()) #Supposedly needed to fix stuff before dense layer\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    if(os.path.exists('../drive/My Drive/checkpoints_attentio/weights-00-0.0274.hdf5')): model.load_weights('../drive/My Drive/checkpoints_attention/weights-200-0.0274.hdf5')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct84xlwRs3m4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, network_input, network_output):\n",
        "    \"\"\" train the neural network \"\"\"\n",
        "    filepath = os.path.abspath(\"../drive/My Drive/checkpoints_attention_mozbet/weights-{epoch:03d}-{loss:.4f}.hdf5\")\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        period=10, #Every epoch\n",
        "        monitor='loss',\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    history = model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list, validation_split=0.2, shuffle=True)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-_ELR9axKD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = None\n",
        "if __name__ == '__main__':\n",
        "    history = train_network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZZJ6ZqYVaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7iNhd6LY-U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtfvDh9Ak7V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(glob.glob(train_path + '**/*nokey.mid', recursive=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}